{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee56c2c-e054-4751-91e3-59d6706e9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from DataCleaning import DataCleaning\n",
    "from EDA import EDA\n",
    "from KFoldCrossValidation import KFoldCrossValidation\n",
    "from LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be935150",
   "metadata": {},
   "source": [
    "### 1. First, the dataset needs to be loaded. For the sake of simplicity, we are gonna use an object for loading and cleaning the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f2cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner = DataCleaning('assets/auto_train.csv')\n",
    "df = data_cleaner.df\n",
    "data_cleaner.show_head(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38158c",
   "metadata": {},
   "source": [
    " ##### 1. a. Checking the missing values\n",
    "To understand the dataset, we need to se whick columns contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d5cfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner.get_nan_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e98603",
   "metadata": {},
   "source": [
    "##### 1. b. Cleaning the dataset of NaN columns\n",
    "We are gonna drop the columns where at least 2/3 of the data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c862ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cleaner.delete_columns_with_nans(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea2eb42",
   "metadata": {},
   "source": [
    "##### 1. c. Check the rest of the missing values in the columns\n",
    "\n",
    "To see what has to be done next in the cleaning phase, wee need to see which columns have missing values. We are gonna do that for both numerical and object columns\n",
    "\n",
    "##### Checking the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_cleaner.df.select_dtypes(include=['number'])\n",
    "numeric_columns_with_nan = numeric_columns.columns[numeric_columns.isna().any()].tolist()\n",
    "numeric_df_with_nan = df[numeric_columns_with_nan]\n",
    "numeric_df_with_nan.info()\n",
    "numeric_dataframe = pd.DataFrame(numeric_df_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6cbe0",
   "metadata": {},
   "source": [
    "##### Checking the object data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = data_cleaner.df.select_dtypes(include=['object'])\n",
    "object_columns_with_nan = object_columns.columns[object_columns.isna().any()].tolist()\n",
    "object_df_with_nan = df[object_columns_with_nan]\n",
    "object_df_with_nan.info()\n",
    "object_dataframe = pd.DataFrame(object_df_with_nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002bef9d",
   "metadata": {},
   "source": [
    "##### 1. d. Filling the missing numeric data\n",
    "\n",
    "The numerical data will be filled with the median of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b04656",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cleaner.fill_nan_with_median(df)\n",
    "object_df_with_nan.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91221701",
   "metadata": {},
   "source": [
    "##### 1. e. Dealing with categorical data\n",
    "\n",
    "First, wee need to se what columns contain categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb114b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner.detect_categorical_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee1d354",
   "metadata": {},
   "source": [
    "We will now fill the categorical data with the most frequent values in those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05043896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_cleaner.fill_nan_with_frequent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02948a8",
   "metadata": {},
   "source": [
    "### 2. EDA\n",
    "\n",
    "##### 2. a. Initializing EDA\n",
    "\n",
    "In this part, we will take care of the EDA part. Because some of the columns are not relevant to the analysis, they will need to be dropped. Moving on, we need to plot de distribution based on the price column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_ids = df[\"id\"]\n",
    "print(car_ids)\n",
    "df = df.drop(columns=['id', 'data', 'url'])\n",
    "EDA_analyzer = EDA(df)\n",
    "EDA_analyzer.show_distribution(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4493e",
   "metadata": {},
   "source": [
    "#### 2. b. Detecting outliers in columns\n",
    "\n",
    "We can see that the distribution does not resemble a classic distribution, so therefore, transformations are required. For this, we need to determine the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148132fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_column = df['pret']\n",
    "numeric_outliers = EDA_analyzer.detect_outliers(df)\n",
    "for col, outliers in numeric_outliers.items():\n",
    "    print(f\"Outliers in column '{col}':\")\n",
    "    print(outliers)\n",
    "    print()\n",
    "column_list = list(numeric_outliers.keys())\n",
    "print(column_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14b863",
   "metadata": {},
   "source": [
    "#### 2. c. Plotting the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ded004",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_analyzer.plot_outliers(df, numeric_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1cd34a",
   "metadata": {},
   "source": [
    "#### 2. d. Applying the logarithmic transformations\n",
    "\n",
    "For getting a better distribution, we need to apply the logarithmic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce491238",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = EDA_analyzer.apply_logarithmic(df, column_list)\n",
    "column_list = EDA_analyzer.get_columns_names(transformed_df)\n",
    "outliers_after = EDA_analyzer.detect_outliers(transformed_df)\n",
    "EDA_analyzer.plot_outliers(transformed_df, outliers_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e47a1",
   "metadata": {},
   "source": [
    "We can see here that the distribution now ressembles a classic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_analyzer.show_distribution(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d33fd",
   "metadata": {},
   "source": [
    "#### 2. e. Plotting the data\n",
    "\n",
    "In order to better understand the data, we need to plot it with the target variable, which in this case is the price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_analyzer.plot_graph(df, 'Km', 'pret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5053d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_analyzer.plot_graph(df, 'Putere', 'pret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_analyzer.plot_graph(df, 'Capacitate cilindrica', 'pret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDA_analyzer.plot_graph(df, 'Consum Urban', 'pret')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b58c5",
   "metadata": {},
   "source": [
    "#### 3. K-Folds Cross Validation\n",
    "\n",
    "In order to predict how well our model will perform, we need to apply cross-validation on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac28a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_cross_validation = KFoldCrossValidation(df)\n",
    "print(df.columns)\n",
    "df = k_fold_cross_validation.df\n",
    "train_buckets = k_fold_cross_validation.leq_range_buckets(\"pret\", [5000, 15000])\n",
    "for bucket in train_buckets:\n",
    "    print(\n",
    "       f\"`Bucket: {bucket}` contains {len(train_buckets[bucket])} samples.\"\n",
    "       f\" Percentage of total: {len(train_buckets[bucket]) / len(df):.2%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = k_fold_cross_validation.make_folds(train_buckets)\n",
    "print(f\"Number of buckets: {len(kfolds)}\")\n",
    "for bucket_name, bucket_kfolds in kfolds.items():\n",
    "  print(f\"Bucket: {bucket_name}\")\n",
    "  for idx, (train_idx, val_idx) in enumerate(bucket_kfolds):\n",
    "    print(f\"Fold: {idx}\")\n",
    "    print(f\"Training indices: {train_idx}\")\n",
    "    print(f\"Validation indices: {val_idx}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd00b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "kfolds = k_fold_cross_validation.make_folds(train_buckets, n_splits=n_folds, shuffle=False)\n",
    "\n",
    "train_dfs, test_dfs = k_fold_cross_validation.get_train_test_folds(train_buckets, kfolds, n_folds)\n",
    "print(f\"Number of folds: {len(train_dfs)}\")\n",
    "for idx, (train_df, test_df) in enumerate(zip(train_dfs, test_dfs)):\n",
    "  print(f\"Fold: {idx}\")\n",
    "  print(f\"Training shape: {train_df.shape}\")\n",
    "  print(f\"Testing shape: {test_df.shape}\")\n",
    "  print()\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55663069",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_cross_validation.plot_correlation_with_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_null = df.isnull().sum().sort_values(ascending=False)\n",
    "percent_null = df.isnull().sum() / df.isnull().count() * 100\n",
    "percent_null = percent_null.sort_values(ascending=False)\n",
    "null_data = pd.concat([total_null, percent_null], axis=1, keys=['Total', 'Percent'])\n",
    "null_data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f8de8f",
   "metadata": {},
   "source": [
    "#### 5. Finally, we run our Regression algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dataframe = df.select_dtypes(include='number')\n",
    "\n",
    "n_folds = 5\n",
    "target = 'pret'\n",
    "feature_columns = num_dataframe.columns.tolist()\n",
    "feature_columns.remove('pret')\n",
    "num_dataframe = num_dataframe.drop(columns='pret')\n",
    "target = df['pret']\n",
    "num_dataframe['id'] = car_ids\n",
    "print(target)\n",
    "print(num_dataframe.columns)\n",
    "features = df[feature_columns]\n",
    "\n",
    "linear_regression = LinearRegression(num_dataframe, features, target, n_folds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.run_regression(\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.run_regression(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.run_regression(\"Extra Trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ec325",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.run_regression(\"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832a2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = 'predictions'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9d2b5",
   "metadata": {},
   "source": [
    "#### 6. Applying operations on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner_test = DataCleaning('assets/auto_test_leaderboard.csv')\n",
    "df_test = data_cleaner_test.df\n",
    "\n",
    "df_test = data_cleaner_test.delete_columns_with_nans(df_test)\n",
    "df_test = data_cleaner_test.fill_nan_with_frequent(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021dd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = data_cleaner.df.select_dtypes(include=['number'])\n",
    "numeric_columns_with_nan = numeric_columns.columns[numeric_columns.isna().any()].tolist()\n",
    "numeric_df_with_nan = df_test[numeric_columns_with_nan]\n",
    "numeric_df_with_nan.info()\n",
    "numeric_dataframe = pd.DataFrame(numeric_df_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b77f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = data_cleaner.df.select_dtypes(include=['object'])\n",
    "object_columns_with_nan = object_columns.columns[object_columns.isna().any()].tolist()\n",
    "object_df_with_nan = df_test[object_columns_with_nan]\n",
    "object_df_with_nan.info()\n",
    "object_dataframe = pd.DataFrame(object_df_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb79a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = data_cleaner.fill_nan_with_median(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_ids_test = df_test[\"id\"]\n",
    "print(car_ids)\n",
    "df_test = df_test.drop(columns=['id', 'data'])\n",
    "\n",
    "num_dataframe = df_test.select_dtypes(include='number')\n",
    "print(num_dataframe.columns)\n",
    "n_folds = 5\n",
    "target = 'pret'\n",
    "feature_columns = num_dataframe.columns.tolist()\n",
    "# feature_columns.remove('pret')\n",
    "# target = df['pret']\n",
    "num_dataframe['id'] = car_ids\n",
    "print(target)\n",
    "\n",
    "new_data_features = df_test[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f39074",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = linear_regression.load_model('Decision Tree_model.pk1')\n",
    "predictions = LinearRegression.predict(loaded_model, new_data_features)\n",
    "\n",
    "new_data_predictions = pd.DataFrame({'id': new_data_features['id'], 'value': predictions})\n",
    "\n",
    "output_filepath = os.path.join(output_dir, 'decision_tree_new_data_predictions.csv')\n",
    "new_data_predictions.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fedd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = linear_regression.load_model('Random Forest_model.pk1')\n",
    "predictions = LinearRegression.predict(loaded_model, new_data_features)\n",
    "\n",
    "new_data_predictions = pd.DataFrame({'id': new_data_features['id'], 'value': predictions})\n",
    "\n",
    "output_filepath = os.path.join(output_dir, 'random_forest_new_data_predictions.csv')\n",
    "new_data_predictions.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = linear_regression.load_model('Extra Trees_model.pk1')\n",
    "predictions = LinearRegression.predict(loaded_model, new_data_features)\n",
    "\n",
    "new_data_predictions = pd.DataFrame({'id': new_data_features['id'], 'value': predictions})\n",
    "\n",
    "output_filepath = os.path.join(output_dir, 'extra_trees_new_data_predictions.csv')\n",
    "new_data_predictions.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = linear_regression.load_model('Gradient Boosting_model.pk1')\n",
    "predictions = LinearRegression.predict(loaded_model, new_data_features)\n",
    "\n",
    "new_data_predictions = pd.DataFrame({'id': new_data_features['id'], 'value': predictions})\n",
    "\n",
    "output_filepath = os.path.join(output_dir, 'gradient_boosting_new_data_predictions.csv')\n",
    "new_data_predictions.to_csv(output_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
